[
  {
    "objectID": "posts/bayesian-mushra/index.html",
    "href": "posts/bayesian-mushra/index.html",
    "title": "Bayesian Ranking & Selection for listening tests",
    "section": "",
    "text": "Sensory evaluation of sound is an essential step in the development of audio and acoustic systems. In sensory evaluation listening tests, assessors listen to different audio systems, such as alternative audio codecs, hardware configurations, loudspeaker layouts, or signal processing methods, and give responses according to the specific listening test task; for example: select the system that appears to have more bass or clarity, or give each system a rating between 0 and 100 for stereo image quality or basic audio quality.\nResults from such tests always involve uncertainty; different listeners may give slightly different ratings to the same system, or even the same listener may give different responses in repeated trials of identical conditions. When interpreting the results and drawing conclusions from the data, this uncertainty is taken into account by statistical analysis. If you’ve ever done statistics on listening test data (or possibly other kinds of sensory evaluation data), you may be familiar with the feeling that the statistical models and methods are not working for you but you are instead working for them—and not getting much in return.\nI suggest that this difficulty with sensory evaluation data and statistics is because the classical statistical testing is geared towards answering different kinds of questions than the ones we’d like to ask about our listening experiment results. But luckily, things can be made easier by using a different statistical framework—ranking & selection (R&S)—where the questions are formulated in ways that fit sensory evaluation tasks perfectly.\nThis article is a Jupyter notebook, rendered to HTML with Quarto. It contains the Python code for running the analysis and creating the figures and tables. The code cells are hidden by default, to keep the text readable—the code cells can be expanded by clicking on them. In addition to standard scientific Python packages, I’ll use STAN for the bayesian inference via its CmdStanPy python interface.\nCode\nimport numpy as np\nfrom scipy import stats\nfrom scipy.special import expit\nimport matplotlib.pyplot as plt\nimport arviz as az\nimport pandas as pd\nimport cmdstanpy as csp\nimport seaborn as sns\n\nplt.rcParams[\"figure.dpi\"] = 144\nsns.set_theme(context=\"notebook\", style=\"whitegrid\", palette=\"Set2\")\n%matplotlib inline"
  },
  {
    "objectID": "posts/bayesian-mushra/index.html#ranking-selection",
    "href": "posts/bayesian-mushra/index.html#ranking-selection",
    "title": "Bayesian Ranking & Selection for listening tests",
    "section": "Ranking & Selection",
    "text": "Ranking & Selection\nIn the statistical framework called ranking and selection (R&S)1,2, we would instead directly ask questions like “which system is on average the best?” and based on the listening test responses arrive at both an answer as well as a measure of confidence for the answer, such as “system 3 is rated best with probability 96.3%”. Or we can rank the systems based on current assessor responses and ask “starting from the top, how many systems \\(K\\) out of total \\(N\\) do I have to pick to be at least 99.9% certain that one of the systems is really the best?”, with the answer being something between 1 (it is very clear which system is the best) and \\(N\\) (we have no idea which is best, so if we include all systems, the best is one of them with 100% confidence).\nOr, as put by Gibbons et al. (1979)3:\n\nMore generally, ranking and selection procedures are statistical techniques for comparing the parameters of some \\(k\\) populations under the assumption that these parameters are not all the same.\n\nIn other words, R&S seems like a perfect match for our sensory evaluation data. We get rid of the artificial null hypotheses and their testing and rejection, and instead go directly at the questions we actually want answered.\nAnother good match for R&S is Bayesian inference, where we would model “the parameters of some \\(k\\) populations” as distributions of random variables, and conduct our statistical analysis using those distributions. This approach is called Bayesian R&S and it is used already in fields like operations research and machine learning. Bringing in the bayesian framework gives us a lot more tools to work with. After formulating our listening test as a bayesian statistical model, we can do bayesian experimental design (BED) to optimise the listening test, for example by using a sequential design, where we pick the next test item based on the information value we would expect to gain by testing each particular item4,5.\nNext, I’ll show how Bayesian R&S could be employed in the statistical analysis of results of the widely-used MUSHRA listening test format."
  },
  {
    "objectID": "posts/bayesian-mushra/index.html#example-data",
    "href": "posts/bayesian-mushra/index.html#example-data",
    "title": "Bayesian Ranking & Selection for listening tests",
    "section": "Example data",
    "text": "Example data\nWe have the results of a MUSHRA listening test that investigated spatial audio quality, in which \\(N = 15\\) listeners \\(k = 1 \\ldots N\\) evaluated \\(M = 6\\) systems \\(j = 1 \\ldots 6\\) (of which system 1 is the reference and system 6 the anchor) and gave them ratings \\(r_{kj} \\in [0,  100]\\). The responses are shown in Figure 2.\nHere we will only consider ratings for a single sound clip, to keep the statistical model simple enough. For actual listening experiment data, where we have multiple conditions and possibly repetitions of items, we would add more parameters to the model, accounting for different experimental conditions and also listener-specific effects.\n\n\nCode\nratings = np.loadtxt(\"01-listening_test_results.csv\", delimiter=\";\")\nparticipants = np.arange(ratings.shape[0], dtype=int) + 1\nparticipants = np.repeat(participants, 6).flatten()\nsystem_ids = np.arange(ratings.shape[1], dtype=int) + 1\nsystems = np.tile(system_ids, (15,1))\nratings = ratings.flatten()\nsystems = systems.flatten()\ndf = pd.DataFrame({\"ratings\": ratings, \"systems\": systems, \"listeners\": participants})\n\nfig, ax = plt.subplots(1,1,figsize=(4,3))\nfig.set_layout_engine('tight')\nfig.subplots_adjust(left=1.5/7, right=5.5/7)\nsns.stripplot(df, x=\"systems\", y=\"ratings\", hue=\"systems\", ax=ax, \n              palette=\"Set2\", legend=False, linewidth=1.0)\nax.set_xlabel(\"System\")\nax.set_ylabel(\"Rating\")\nax.set_title(r\"Ratings $r_{kj}$\");\n\n\n\n\n\n\n\n\nFigure 2: Responses from 15 participants in a MUSHRA test with four systems under test (systems 2–5), as well as a hidden reference (system 1) and anchor (system 6).\n\n\n\n\n\nWe see that the reference system 1 has been correctly identified by the listeners every time, and apart from two answers also the anchor 6 has been consistently rated at 0. Systems 4 and 5 perform clearly worse than systems 2 and 3. This kind of data is a bit cumbersome if we would for example want to do ANOVA directly on the raw ratings; the responses are limited between 0 and 100, so the ratings tend to “pack” at the extremes, and the assumption of homogeneity of variances (of the residuals) is not fulfilled. As with the bayesian model in the next section, also ANOVA can be extended with GLMs and other approaches that mitigate these issues, but with ANOVA, in the end we would still be doing hypothesis testing and not R&S."
  },
  {
    "objectID": "posts/bayesian-mushra/index.html#prior-predictive-checks",
    "href": "posts/bayesian-mushra/index.html#prior-predictive-checks",
    "title": "Bayesian Ranking & Selection for listening tests",
    "section": "Prior predictive checks",
    "text": "Prior predictive checks\nTo have an idea of what the model (1) does, and whether the priors we have chosen make any sense, it is a good idea to take a look at the prior predictive distributions. Here, no data is yet analysed—we only want to take the prior distributions as they are and see how the parameters and predictive distributions for \\(y_{kj}\\) look like with those values. We will now draw random samples from the prior and hyperprior distributions and use those random values as parameters for drawing samples from the beta distribution.\nThe point is to see if the distributions of \\(\\mu_j\\) and \\(y_{kj}\\) make sense given our experimental design. It could be for example that our model would a priori result in distributions that are focused on the extreme ends of the scale: to 0 and 1, without any probability mass in between. This would mean that before seeing any data, the model expects that all ratings would most likely be either of value 0 or 100 and that it would be extremely rare to see any ratings of intermediate values. Clearly, this is not what we expect, but rather we assume that ratings should be quite evenly distributed across the scale—and our prior predicitive distributions should then reflect this.\n\n\nCode\nN = 10000\nnp.random.seed(123)\n\n# draw N samples from the prior distributions\nmu0 = stats.norm.rvs(0, 1, size=N)\n# the alpha_j prior depends on the hyperprior sigma_alpha, so draw them first ...\nsigma_alpha = stats.invgamma.rvs(3, size=N)\n# ... and use the sigma_alpha values to draw alpha_j\nalpha_j = stats.norm.rvs(0, sigma_alpha, size=N)\n# note the difference between scipy and stan parameters. Here scale=10, and in stan the 2nd parameter is 0.1 = 1/scale\nphi_j = stats.gamma.rvs(2, scale=10, size=N)\n\n# we have logit(mu_j) = mu0 + alpha_j, so take the inverse logit (in scipy, expit) to get mu_j\nmu_j = expit(mu0 + alpha_j)\n# draw values for y based on the priors\ny_kj = stats.beta.rvs(mu_j * phi_j, (1 - mu_j)*phi_j, size=N)\n\n\nFirst, let’s look at how the beta distributions look like. Here we draw 15 random pairs of \\(\\mu_j\\) and \\(\\phi_j\\) parameters from the priors and generate 10000 samples from the beta distribution with those values for predictions of possible \\(y_{kj}\\).\n\n\nCode\nsns.set_style(\"white\", rc={\"axes.edgecolor\": \"0.8\"})\n_, axs = plt.subplots(3,5, figsize=(7, 5), sharey=True, gridspec_kw={\"hspace\": 0.2})\n\nfor axi, ax in enumerate(axs.flatten()):\n    y_fix = stats.beta.rvs(mu_j[axi] * phi_j[axi], (1 - mu_j[axi])*phi_j[axi], size=N)\n    ax.hist(y_fix, bins=40, density=True, histtype=\"stepfilled\")\n    ax.set_xlim((-0.01,1.01))\n    ax.set_xticks([0,1])\n    ax.tick_params(labelsize=8)\n    ax.xaxis.set_visible(True)\n    ax.yaxis.set_visible(False)\n    ax.set_title(rf\"$\\mu_j = {mu_j[axi]:.2f}, \\phi_j = {phi_j[axi]:.2f}$\", \n                 fontdict={\"fontsize\": 8})\n\n\n\n\n\n\n\n\nFigure 3: Beta distributions with shape parameters \\(\\mu_j\\) and \\(\\phi_j\\) drawn from the prior distributions\n\n\n\n\n\nWe see that the mean values \\(\\mu_j\\) go from 0.12 to 0.88 and the precisions \\(\\phi_j\\) from 2.45 (very broad distribution in upper right corner) to 50.2 (narrow distribution in lower right corner). The distributions seem plausible in that there are no cases where the responses would for example be concentrated on some single value, and there is also good variability in the shapes of the distributions such that many different kinds of response profiles are plausible with the priors. Scale effects also appear to be taken into account, since as the mean gets closer to either extreme, the values start to “pack” at the boundaries, as we would also expect to happen for the actual ratings.\nLooking at the prior distribution for \\(\\mu_j\\), we see that the means are more concentrated towards the center of the scale. This could be interpreted as a conservative prior; it is less likely to have systems with means at the very extremes. If instead of looking at the distributions of \\(y_{kj}\\) obtained with fixed parameters as in Figure 3, we look at how the \\(y_{kj}\\) are distributed across all possible prior draws, we see that the simulated ratings are quite evenly distributed across the full scale with a bit more values in the middle, and also clear peaks at the extreme ends of the scale.\nAs very general prior assumptions, these distributions look ok—people are usually more likely to give intermediate ratings that extreme values—but depending on the specific situation, it could make sense to fine-tune the priors. In a MUSHRA test, the midpoint of the scale does not actually correspond to some neutral value. Instead, the labels on the scale are “excellent”, “good”, “fair”, “poor”, and “bad”, and thus the midpoint would fall within the range labelled as “fair”. There can be language factors or cultural factors which might have slight effects regarding where a “neutral” answer would go on this scale; does “good” mean “ok”, and how much worse is “bad” than “poor”? So, depending on the context, one might adjust the priors for example so that higher ratings would be more likely. This would not have a dramatic effect on the results, however, since as actual observations are fitted to the model, strong evidence will always outweigh these weakly informative priors.\n\nCode\nsns.set_style(\"ticks\")\nsns.set_palette(\"Set2\")\nfig1, ax = plt.subplots(1,1, figsize=(3,2))\nax.hist(mu_j, bins=40, density=True)\nax.set_title(r\"Prior predictive distribution for $\\mu_j$\")\nsns.despine()\n\nfig2, ax = plt.subplots(1,1, figsize=(3,2))\nax.hist(y_kj, bins=40, density=True)\nax.set_title(r\"Prior predictive distribution for $y_{kj}$\")\nsns.despine()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Distribution of system-specific means for beta distribution\n\n\n\n\n\n\n\n\n\n\n\n(b) Distribution of predicted observations\n\n\n\n\n\n\n\nFigure 4: Prior predictive distributions"
  },
  {
    "objectID": "posts/bayesian-mushra/index.html#stan-implementation",
    "href": "posts/bayesian-mushra/index.html#stan-implementation",
    "title": "Bayesian Ranking & Selection for listening tests",
    "section": "STAN implementation",
    "text": "STAN implementation\nThe model in Equation 1 was implemented using STAN. The model specification is given below. STAN uses the MCMC method (Markov Chain Monte Carlo) to numerically solve the posterior distribution — by simulating a sufficient number of samples from the posterior distribution implied by the model, the distribution can be approximated to the desired accuracy.\n// Hierarchical Beta regression for MUSHRA scores\n\ndata {\n  int&lt;lower=1&gt; N;                   // total observations\n  int&lt;lower=1&gt; S;                   // number of systems\n  array[N] int&lt;lower=1&gt; system_id;  // system id of each observation\n\n  array[N] real&lt;lower=0,upper=1&gt; y; // rescaled scores in (0,1)\n}\n\nparameters {\n  real mu_0;                           // global intercept on logit scale\n  array[S] real alpha_j;               // system offsets (logit scale)\n  real&lt;lower=0&gt; sigma_alpha;           // sd for system effects\n  array[S] real&lt;lower=0&gt; phi_j;        // Beta precision\n}\n\ntransformed parameters {\n  // linear predictor on logit scale for each observation\n  vector[N] eta;\n  for (n in 1:N) {\n    eta[n] = mu_0 + alpha_j[system_id[n]];\n  }\n}\n\nmodel {\n  // Priors\n  mu_0 ~ normal(0, 1);\n  alpha_j ~ normal(0, sigma_alpha);\n  sigma_alpha ~ inv_gamma(3, 1);\n  phi_j ~ gamma(2, 0.1);\n\n  // Likelihood: parameterize Beta by mean mu (inv_logit(eta)) and precision phi\n  for (n in 1:N) {\n    real mu_j = inv_logit(eta[n]);\n    y[n] ~ beta(mu_j * phi_j[system_id[n]], (1 - mu_j) * phi_j[system_id[n]]);\n  }\n}"
  },
  {
    "objectID": "posts/bayesian-mushra/index.html#model-fitting",
    "href": "posts/bayesian-mushra/index.html#model-fitting",
    "title": "Bayesian Ranking & Selection for listening tests",
    "section": "Model fitting",
    "text": "Model fitting\nWe then take the STAN model above and fit the actual observations (see Figure 2) to it. First the ratings \\(r_{kj}\\) are scaled to the interval [0, 1] and those values, along with the information about the system for which that rating was given, are passed to the STAN model. The MCMC runs four separate chains, which are first sampled for some time to warm up and stabilise the chains, and then 5000 draws are taken from each Markov chain for the actual posterior draws.\n\n\nCode\nmodel = csp.CmdStanModel(stan_file=\"mushra_simple.stan\")\n\n# rescale raw scores [0,100] to [0,1] for beta regression\neps = 0.5\ny = (df[\"ratings\"] + eps) / (100 + 2*eps)\n\ndata = {\"N\": df.shape[0], \"S\": df[\"systems\"].unique().shape[0], \n        \"system_id\": df[\"systems\"], \"y\": y}\nsample = model.sample(data = data, seed = 121,\n                      iter_warmup = 5000, iter_sampling = 5000,\n                      show_progress = False, show_console = False, fixed_param=False)\n\n\nOnce the MCMC has completed, we can examine the results. First, we check that the simulation chains have converged to the same values for the parameters. This is indicated by the value R_hat in the following table, and quoting STAN documentation, the R_hat convergence diagnostic …\n\n[…] compares the between- and within-chain estimates for model parameters and other univariate quantities of interest. If chains have not mixed well (ie, the between- and within-chain estimates don’t agree), R-hat is larger than 1. We recommend running at least four chains by default and only using the sample if R-hat is less than 1.05.\n\nAll parameters and hyperparameters have an R_hat of 1.0, so all good so far.\n\n\nCode\ndf_summary = sample.summary(sig_figs=2)\ndf_summary.iloc[:15,:]\n\n\n\n\n\n\n\n\n\nMean\nMCSE\nStdDev\nMAD\n5%\n50%\n95%\nESS_bulk\nESS_tail\nESS_bulk/s\nR_hat\n\n\n\n\nlp__\n130.000\n0.0320\n2.70\n2.60\n130.00\n130.000\n140.000\n7200.0\n12000.0\n89.0\n1.0\n\n\nmu_0\n-0.067\n0.0110\n0.69\n0.68\n-1.20\n-0.072\n1.100\n3800.0\n6100.0\n47.0\n1.0\n\n\nalpha_j[1]\n4.600\n0.0110\n0.74\n0.72\n3.30\n4.600\n5.800\n4300.0\n6800.0\n53.0\n1.0\n\n\nalpha_j[2]\n0.900\n0.0110\n0.73\n0.72\n-0.30\n0.900\n2.100\n4100.0\n6900.0\n51.0\n1.0\n\n\nalpha_j[3]\n0.620\n0.0110\n0.72\n0.71\n-0.56\n0.620\n1.800\n4000.0\n6300.0\n50.0\n1.0\n\n\nalpha_j[4]\n-1.200\n0.0120\n0.74\n0.73\n-2.50\n-1.200\n-0.028\n4200.0\n6500.0\n52.0\n1.0\n\n\nalpha_j[5]\n-2.100\n0.0120\n0.75\n0.73\n-3.30\n-2.000\n-0.820\n4100.0\n6900.0\n51.0\n1.0\n\n\nalpha_j[6]\n-3.000\n0.0120\n0.78\n0.76\n-4.30\n-3.000\n-1.800\n4400.0\n7400.0\n55.0\n1.0\n\n\nsigma_alpha\n2.400\n0.0056\n0.68\n0.57\n1.50\n2.200\n3.600\n19000.0\n13000.0\n240.0\n1.0\n\n\nphi_j[1]\n77.000\n0.2000\n30.00\n28.00\n35.00\n73.000\n130.000\n19000.0\n12000.0\n240.0\n1.0\n\n\nphi_j[2]\n4.300\n0.0087\n1.30\n1.30\n2.40\n4.200\n6.800\n23000.0\n14000.0\n280.0\n1.0\n\n\nphi_j[3]\n6.500\n0.0130\n2.00\n2.00\n3.60\n6.300\n10.000\n23000.0\n14000.0\n290.0\n1.0\n\n\nphi_j[4]\n3.600\n0.0080\n1.20\n1.10\n1.90\n3.500\n5.800\n21000.0\n13000.0\n260.0\n1.0\n\n\nphi_j[5]\n6.800\n0.0160\n2.40\n2.30\n3.40\n6.500\n11.000\n22000.0\n12000.0\n270.0\n1.0\n\n\nphi_j[6]\n10.000\n0.0300\n4.20\n4.00\n4.30\n9.500\n18.000\n18000.0\n12000.0\n220.0\n1.0\n\n\n\n\n\n\n\nWe can also look at the MCMC samples with ArviZ (Figure 5), which is a visual tool for working with bayesian models in Python. On the right panel, there are the 5000 individual draws for each chain and on the left panel, the distribution of those drawn values.\n\n\nCode\naz.style.use(\"arviz-doc\")\naz.plot_trace(sample, figsize=(5, 7), backend_kwargs={\"dpi\": 144})\n\nphi_draws = sample.stan_variable(\"phi_j\")\n\nmu0_draws = sample.stan_variable(\"mu_0\")\nalpha_draws = sample.stan_variable(\"alpha_j\")\nmu_draws = expit(mu0_draws[:, np.newaxis] + alpha_draws);\n\n\n\n\n\n\n\n\nFigure 5: MCMC traces for model parameters\n\n\n\n\n\nLooking at the draws for \\(\\mu_0\\) (labeled as mu_0) there seems to be only one curve on the left, but it is just that the distributions for the four chains agree so well that they are basically overlaid and appear as a single curve. This is as we would hope—if we would get diverging simulation chains and different distributions for the different chains, it would mean that something’s unstable and we couldn’t trust the results. For \\(\\alpha_j\\), \\(\\phi_j\\) and eta, the plots show individual curves for each system-specific distribution. In the next step of the analysis, the values of eta are then mapped with the inverse logit to the [0, 1] range, giving us our \\(\\mu_j\\)."
  },
  {
    "objectID": "posts/bayesian-mushra/index.html#posterior-predictive-checks",
    "href": "posts/bayesian-mushra/index.html#posterior-predictive-checks",
    "title": "Bayesian Ranking & Selection for listening tests",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\nWhen we have the full model fitted with data, we’d first like to check that the model makes sense. As we did with the priors, we will now first look at what the full model actually does with the parameter distributions that we’ve obtained. In particular, we want to be sure that the model fits with our observations and has not converged to some strange combination of parameter values.\nSince the parameter values are drawn from a distribution, we can take each draw of parameters and simulate predictions \\(\\tilde{y}\\) for each combination. But it’s also possible to take point estimates of the parameter distributions and use those fixed values. Here I’ll use the posterior means for each parameter.\n\n\nCode\nplt.rcParams[\"figure.dpi\"] = 144\nsns.set_theme(context=\"notebook\", style=\"whitegrid\", palette=\"Dark2\")\nfig, axs = plt.subplots(6,1, sharex=True, figsize=(3,6), gridspec_kw={\"hspace\": 0})\n\nfor i in range(6):\n\n    mu_i = mu_draws[:, i].mean()\n    phi_i = phi_draws[:, i].mean()\n    a = mu_i * phi_i\n    b = (1 - mu_i) * phi_i\n\n    x = np.linspace(\n        stats.beta.ppf(0.001, a, b), \n        stats.beta.ppf(0.999, a, b), \n        100\n    )\n    x_samples = stats.beta.pdf(x, a, b)\n    axs[i].fill_between(100*x, x_samples / x_samples.max(), \n                        facecolor=f\"C{i}\", alpha=0.3, \n                        label=r\"$p(\\tilde{y}|\\boldsymbol{y})$\")\n    axs[i].plot(100*x, x_samples / x_samples.max(), color=f\"C{i}\", \n                alpha=0.7, lw=1.5)\n\n    rtgs = df[df[\"systems\"] == i+1]\n    xs = rtgs[\"ratings\"].values\n    axs[i].plot(xs, stats.uniform.rvs(-0.2, 0.2, size=len(xs)), 'o', \n                markersize=3, c=f\"C{i}\", alpha=0.8, label=r\"$y_{kj}$\", \n                mec=\"0.3\", mew=1.0)\n    axs[i].text(0.1, 0.8, f\"System {i+1}\", transform=axs[i].transAxes, \n                verticalalignment=\"top\", horizontalalignment=\"left\")\n    axs[i].vlines([0, 20, 40, 60, 80, 100], 0, 1, \n                  transform=axs[i].get_xaxis_transform(), lw=1, \n                  colors=\"0.85\", zorder=0)\n\n    axs[i].set_ylim((-0.3, 1.3))\n    axs[i].set_xlim((-5, 105))\n    axs[i].set_yticks([])\n    axs[i].spines[\"top\"].set_visible(False)\n    axs[i].spines[\"left\"].set_visible(False)\n    axs[i].spines[\"right\"].set_visible(False)\n    if i &lt; 5:\n        axs[i].tick_params(length=0)\n        axs[i].spines[\"bottom\"].set_visible(False)\n    else:\n        axs[i].spines[\"bottom\"].set_visible(True)\n        axs[i].spines[\"bottom\"].set_color(\"0.7\")\n        axs[i].set_xlabel(\"Rating\")\n        _ = axs[i].set_xticks([0, 20, 40, 60, 80, 100])\n        axs[i].legend()\nfig.suptitle(r\"Posterior predictive distributions $p(\\tilde{y}|\\boldsymbol{y})$\", \n                 fontsize=10);\n\n\n\n\n\n\n\n\nFigure 6: Posterior predictive distributions for each system. Here, the values of \\(y\\) are scaled to [0, 100] for comparison with original listening test ratings \\(r_{kj}\\).\n\n\n\n\n\nThe distributions plotted as shaded areas seem to fit nicely with the observations, spanning the range of given ratings. The scale end effects are shown very nicely; where there are multiple ratings at either extreme, the distribution is “pulled” towards the edge."
  },
  {
    "objectID": "posts/bayesian-mushra/index.html#rs-with-posteriors",
    "href": "posts/bayesian-mushra/index.html#rs-with-posteriors",
    "title": "Bayesian Ranking & Selection for listening tests",
    "section": "R&S with posteriors",
    "text": "R&S with posteriors\nNow that we have the bayesian model for our listening test data and the posterior distributions for the model parameters, actually the R&S part is very very simple. Thinking back on the Gibbons et al. quote earlier: “… ranking and selection procedures are statistical techniques for comparing the parameters of some \\(k\\) populations …”, we now have in fact a full statistical model describing those parameters, and can calculate the R&S statistics from the posterior draws.\nAs an example, if we look at the posterior distributions of \\(\\mu_j\\) (Figure 7), we see that the distribution of \\(\\mu_2\\) seems to have most mass above the others (apart from the reference system \\(\\mu_1\\)). But how confident can we be in stating that system 2 is on average the second best? We calculate from the posterior draws \\(\\mathrm{Pr}(\\mu_2 &gt; \\mathrm{max}_{j \\neq 3} \\mu_j)\\), or in practice count the number of posterior draws where the value of \\(\\mu_2\\) ranks second and normalise with the total number of draws. Basically it’s a competition between \\(\\mu_3\\) and \\(\\mu_2\\), and on each posterior draw, we get a single value from each distribution; as the distributions overla, sometimes the value for \\(\\mu_3\\) is higher than the one for \\(\\mu_2\\), but more often \\(\\mu_2\\) is higher than \\(\\mu_3\\). The result is that system 2 ranks second best in 81.5% of the posterior draws (and system 3 18.5% of the time).\n\n\nCode\nbest_system = np.argmax(mu_draws[:, 1:5], axis=1) + 2\nvals, counts = np.unique(best_system, return_counts=True)\n\nfor v, c in zip(vals, counts):\n    print(f\"System {v} is 2nd best with probability {100*c/mu_draws.shape[0]:.1f}%\")\n\nsns.set_theme(context=\"notebook\", style=\"darkgrid\", palette=\"Dark2\")\n_, ax = plt.subplots(1,1,figsize=(4,3))\nfor i in range(6):\n    h, bins = np.histogram(100*mu_draws[:, i], bins=50)\n    ax.stairs(h, bins, fill=True, ls=\"-\", ec=\"0.1\", alpha=0.5, lw=1.0)\n    system_mean = 100*mu_draws[:,i].mean()\n    if i == 0:\n        system_mean = 90\n    ax.text(system_mean, 2300, f\"{i+1}\", horizontalalignment=\"center\", c=f\"C{i}\", \n            fontweight=\"extra bold\")\nax.text(0, 2300, \"$j$:\", horizontalalignment=\"left\")\n#sns.despine(ax=ax, left=True, trim=True)\nax.set_yticks([])\nax.set_xlim((0,100))\n\nax.set_xlabel(\"Rating\")\nax.set_title(r\"Posterior distributions for $\\mu_j$\")\n\nplt.savefig(\"preview.png\");\n\n\nSystem 2 is 2nd best with probability 81.5%\nSystem 3 is 2nd best with probability 18.5%\n\n\n\n\n\n\n\n\nFigure 7: Posterior distributions for system-specific mean ratings \\(\\mu_j\\), scaled from [0, 1] to the original [0, 100] range.\n\n\n\n\n\nIt is now up to the experimenter to decide whether the 81.5% probability is enough for stating that system 2 is the best among the systems under test. There is no statistical test for rejection or acceptance of a hypothesis. One can decide in advance what level of evidence will be considered credible enough.\nWe could also calculate different R&S statistics from the posteriors. For example, we could now say with very high confidence (in 100% of posterior draws) either system 2 or system 3 is the second-best."
  },
  {
    "objectID": "posts/bayesian-mushra/index.html#indifference-zone-rs",
    "href": "posts/bayesian-mushra/index.html#indifference-zone-rs",
    "title": "Bayesian Ranking & Selection for listening tests",
    "section": "Indifference-zone R&S",
    "text": "Indifference-zone R&S\nAn interesting R&S formulation is the so-called indifference-zone (IZ) -approach, where make a decision that when the difference between mean ratings of two systems is less than some pre-defined value—let’s say 5 points on the 0–100 scale—then it means that there is in practice no real difference between the systems. So if we had a situation where our listening test top-two systems had mean ratings \\(\\mu_A\\) = 82 and \\(\\mu_B\\) = 85, and all other systems had ratings below 40, then we would not be making a huge mistake by stating that system A is the best; a difference of three points in a MUSHRA test is in practice degligible and you’d be just as correct in announcing either A or B as the best. So we could say that in a MUSHRA test we would be indifferent to differences less than 5 points and do our R&S analysis accordingly.\nFor our data, we would only need to consider systems 2 and 3. For the IZ analysis we would first set \\(\\delta_{IZ} = 5\\), and then calculate for system 2: \\[p_{IZ,2} = \\mathrm{Pr}(\\mu_2 &gt; \\mu_3), \\quad \\mathrm{if} \\, |\\mu_2 - \\mu_3| &gt; \\delta_{IZ}\\]\n\n\nCode\n# scale delta_IZ to [0, 1]\ndelta_IZ = 5/100\n\nidxs = np.where(np.abs(mu_draws[:,1] - mu_draws[:,2]) &gt; delta_IZ)[0]\nprint(f\"p_IZ,2 = {100*np.sum(mu_draws[idxs,1] &gt; mu_draws[idxs,2])/idxs.shape[0]:.1f}%\")\n\n\np_IZ,2 = 90.6%\n\n\nHere we see that the probability of system 2 being ranked above system 3 increased from the 81.5% earlier to 90.6% with the IZ formulation. Here we are only interested in being correct, if the real mean difference between systems is at least 5 points. If it would turn out to be less than 5 points, then we don’t really care which system is labeled as best.\nDepending on the precise experimental setup and research questions, the IZ formulation can seem either a bit weird or very useful. In the MUSHRA context, it’s not immediately obvious where we might use it, although the formulation itself feels intuitive—small differences are not very important. But if instead of MUSHRA scores we would be modeling for example a financial utility function, and we would have to pick one system that would maximise our utility (e.g., best arm identification), then the IZ formulation seems quite smart. For two systems that are very close to each other, it doesn’t matter that much which one we pick—the utility will be about the same for both—but if there is a large difference between the utilities, then in those cases it really is important to pick the one that results in a higher utility rather than a lower utility, and so it makes sense to focus mostly on begin correct in those cases where the selection makes a bigger difference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Bayesian Ranking & Selection for listening tests\n\n\n\nbayesian\n\npython\n\nstatistics\n\n\n\n\n\n\n\n\n\nNov 4, 2025\n\n\nPetteri Hyvärinen\n\n\n\n\n\nNo matching items"
  }
]